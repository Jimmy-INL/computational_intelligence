{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Part 2 -- Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is the task of gathering samples into groups of similar\n",
    "samples according to some predefined similarity or distance (dissimilarity)\n",
    "measure, such as the Euclidean distance.\n",
    "\n",
    "<img width=\"60%\" src='figures/clustering.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will explore a basic clustering task on some synthetic and real-world datasets.\n",
    "\n",
    "Here are some common applications of clustering algorithms:\n",
    "\n",
    "- Compression for data reduction\n",
    "- Summarizing data as a reprocessing step for recommender systems\n",
    "- Similarly:\n",
    "   - grouping related web news (e.g. Google News) and web search results\n",
    "   - grouping related stock quotes for investment portfolio management\n",
    "   - building customer profiles for market analysis\n",
    "- Building a code book of prototype samples for unsupervised feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a simple, 2-dimensional, synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scatter plot above, we can see three separate groups of data points and we would like to recover them using clustering -- think of \"discovering\" the class labels that we already take for granted in a classification task.\n",
    "\n",
    "Even if the groups are obvious in the data, it is hard to find them when the data lives in a high-dimensional space, which we can't visualize in a single histogram or scatterplot.\n",
    "\n",
    "This is the class separation we would like to recover, using only the feature data and not the labels of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering algorithm we'll look at is called \"K means.\" This is because the algorithm works by fitting *k* center of mean points over the data. We'll choose `k=3` at first. To begin with, 3 random data points are chosen to be the first centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "# 1. Randomly choose clusters\n",
    "n_clusters = 3\n",
    "\n",
    "rng = np.random.RandomState(random_state)\n",
    "i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "centers = X[i]\n",
    "print(i)\n",
    "print(centers)\n",
    "\n",
    "labels = pairwise_distances_argmin(X, centers)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are decided based on which center (mean) is closest to the point. Note that this plot is in two dimensions, but distance can be calculated in any number of dimensions, over all features of the dataset.\n",
    "\n",
    "Once the current labels are calculated, re-calculate the center points as the mean position over all points with the associated label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centers = np.array([X[labels == i].mean(0) for i in range(n_clusters)])\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.3);\n",
    "plt.scatter(new_centers[:, 0], new_centers[:, 1], c='black', s=200, alpha=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this process of labeling the data and recalculating the centers until the centers no longer move:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # 2a. Assign labels based on closest center\n",
    "    labels = pairwise_distances_argmin(X, centers)\n",
    "    \n",
    "    # 2b. Find new centers from means of points\n",
    "    new_centers = np.array([X[labels == i].mean(0)\n",
    "                            for i in range(n_clusters)])\n",
    "    \n",
    "    # 2c. Check for convergence\n",
    "    if np.all(centers == new_centers):\n",
    "        break\n",
    "    centers = new_centers\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn makes this simple for us by providing a KMeans class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the cluster labels either by calling fit and then accessing the \n",
    "``labels_`` attribute of the K means estimator, or by calling ``fit_predict``.\n",
    "Either way, the result contains the ID of the cluster that each point is assigned to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the assignments that have been found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=labels);\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='black', s=200, alpha=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are probably satisfied with the clustering results. But in general we might want to have a more quantitative evaluation. How about comparing our cluster labels with the ground truth we got when generating the blobs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print('Accuracy score:', accuracy_score(y, labels))\n",
    "print(confusion_matrix(y, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Why is the clustering result 0.0 and not 1.0?\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we recovered the partitioning of the data into clusters perfectly, the cluster IDs we assigned were arbitrary,\n",
    "and we can not hope to recover them. Therefore, we must use a different scoring metric, such as ``adjusted_rand_score``, which is invariant to permutations of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "adjusted_rand_score(y, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the \"short-comings\" of K-means is that we have to specify the number of clusters, which we often don't know *apriori*. For example, let's have a look what happens if we set the number of clusters to 2 in our synthetic 3-blob dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=random_state)\n",
    "labels = kmeans.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels);\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='black', s=200, alpha=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "# Incorrect number of clusters\n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.title(\"Incorrect Number of Blobs\")\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\n",
    "plt.title(\"Anisotropicly Distributed Blobs\")\n",
    "\n",
    "# Different variance\n",
    "X_varied, y_varied = make_blobs(n_samples=n_samples,\n",
    "                                cluster_std=[1.0, 2.5, 0.5],\n",
    "                                random_state=random_state)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
    "plt.title(\"Unequal Variance\")\n",
    "\n",
    "# Unevenly sized blobs\n",
    "X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\n",
    "y_pred = KMeans(n_clusters=3,\n",
    "                random_state=random_state).fit_predict(X_filtered)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\n",
    "plt.title(\"Unevenly Sized Blobs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Notable Clustering Routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are two well-known clustering algorithms. \n",
    "\n",
    "- `sklearn.cluster.KMeans`: <br/>\n",
    "    The simplest, yet effective clustering algorithm. Needs to be provided with the\n",
    "    number of clusters in advance, and assumes that the data is normalized as input\n",
    "    (but use a PCA model as preprocessor).\n",
    "- `sklearn.cluster.MeanShift`: <br/>\n",
    "    Can find better looking clusters than KMeans but is not scalable to high number of samples.\n",
    "- `sklearn.cluster.DBSCAN`: <br/>\n",
    "    Can detect irregularly shaped clusters based on density, i.e. sparse regions in\n",
    "    the input space are likely to become inter-cluster boundaries. Can also detect\n",
    "    outliers (samples that are not part of a cluster).\n",
    "- `sklearn.cluster.AffinityPropagation`: <br/>\n",
    "    Clustering algorithm based on message passing between data points.\n",
    "- `sklearn.cluster.SpectralClustering`: <br/>\n",
    "    KMeans applied to a projection of the normalized graph Laplacian: finds\n",
    "    normalized graph cuts if the affinity matrix is interpreted as an adjacency matrix of a graph.\n",
    "- `sklearn.cluster.Ward`: <br/>\n",
    "    Ward implements hierarchical clustering based on the Ward algorithm,\n",
    "    a variance-minimizing approach. At each step, it minimizes the sum of\n",
    "    squared differences within all clusters (inertia criterion).\n",
    "\n",
    "Of these, Ward, SpectralClustering, DBSCAN and Affinity propagation can also work with precomputed similarity matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cluster_comparison.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: IRIS clustering</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Perform K-means clustering on the iris data, searching for 3 clusters. How well does K-means match up with the actual clusters? Try clustering with other numbers.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "x_index = 0\n",
    "y_index = 1\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for label, color in zip(range(len(iris.target_names)), colors):\n",
    "    plt.scatter(iris.data[iris.target==label, x_index], \n",
    "                iris.data[iris.target==label, y_index],\n",
    "                label=iris.target_names[label],\n",
    "                c=color)\n",
    "\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load sols/02_iris_clustering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score(iris.target, labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
