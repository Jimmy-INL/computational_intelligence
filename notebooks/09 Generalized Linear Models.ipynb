{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple way to construct a model of our data is to assume that the target, `y`, is a linear combination of the data, `X`. That is to say that we can multiply each feature of `X_i` by some weight `w_i` (and add a constant `b`) to get our original `y`:\n",
    "\n",
    "$$y = X w + b$$\n",
    "\n",
    "Since this calculated `y` is sure to be different from our original results, lets call it `h`. To determine how inaccurate the model is, simply subtract each ``h`` from the corresponding correct ``y``; if you square the result, you'll get the absolute error, ignoring the sign. Add this error over all the samples and divide by the number of samples to get the Mean Squared Error:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i}^{n} (h_i - y_i)^2 = \\frac{1}{n} \\sum_{i}^{n} (X_i w + b - y_i)^2$$\n",
    "\n",
    "The goal of learning our model then becomes minimizing this error by finding the best `w` and `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To see this demonstrated, let's fit a line to a `sin` wave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.linspace(-3, 3, 100)\n",
    "rng = np.random.RandomState(42)\n",
    "y = np.sin(4 * x) + x + rng.uniform(size=len(x))\n",
    "X = x[:, np.newaxis]\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, y)\n",
    "print('Score: {}, w: {}, b: {}'.format(regressor.score(X, y), regressor.coef_, regressor.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "min_pt = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], color='g')\n",
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "h = regressor.predict(X)\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.scatter(X, h, label='prediction')\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], color='g', label='fit')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This method has been using the 'Ordinary Least squares' method to minimize the MSE:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i}^{n} (h_i - y_i)^2 = \\frac{1}{n} \\sum_{i}^{n} (X_i w + b - y_i)^2$$\n",
    "\n",
    "Other linear models are available which use different error metrics. One example is ElasticNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic = ElasticNet()\n",
    "elastic.fit(X, y)\n",
    "print('Least squares: {}, Elastic Net: {}'.format(regressor.score(X, y), elastic.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "min_pt_e = X.min() * elastic.coef_[0] + elastic.intercept_\n",
    "max_pt_e = X.max() * elastic.coef_[0] + elastic.intercept_\n",
    "\n",
    "h = regressor.predict(X)\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], color='g', label='ols')\n",
    "plt.plot([X.min(), X.max()], [min_pt_e, max_pt_e], color='r', label='elastic')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: Linear regression on the diabetes dataset</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Download ``02_diabetes_linear.py`` from the course website. Change the feature that linear regression is used on and see if any feature can be accurately described using linear regression.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#%load exercises/02_diabetes_linear.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By drawing lines which splits the dataset, we can use this method for classification. However, we use a function, called a logistic function, to better separate the classes. This is therefore known as \"logistic regression\", which, despite its name, is a classification method. Here, we'll look at a method called \"One-vs-Rest logistic regression\" where a line is fit for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# make 3-class dataset for classification\n",
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
    "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "for i in range(len(centers)):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='ovr').fit(X, y)\n",
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict(X)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(centers)):\n",
    "    idx = np.where(prediction == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1])\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "def line(c, x0):\n",
    "    return (-(x0 * clf.coef_[c, 0]) - clf.intercept_[c]) / clf.coef_[c, 1]\n",
    "    \n",
    "for c in clf.classes_:\n",
    "    plt.plot([x_min, x_max], [line(c, x_min), line(c, x_max)])\n",
    "        \n",
    "axes = plt.gca()\n",
    "axes.set_xlim([x_min, x_max])\n",
    "axes.set_ylim([y_min, y_max])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "h = .01\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
    "plt.axis('tight')\n",
    "for i in range(len(centers)):\n",
    "    idx = np.where(prediction == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1])\n",
    "for c in clf.classes_:\n",
    "    plt.plot([x_min, x_max], [line(c, x_min), line(c, x_max)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "301f48e84b80462489897246c47fe3ad",
   "lastKernelId": "8ea6ccae-578f-4a86-a868-da9e1e9e3883"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
