{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_plot(data):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(data.data)\n",
    "    data_pca = pca.transform(data.data)\n",
    "\n",
    "    for label in range(len(data.target_names)):\n",
    "        plt.scatter(data_pca[data.target==label, 0],\n",
    "                    data_pca[data.target==label, 1],\n",
    "                    label=data.target_names[label])\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning consists in learning the link between two datasets: the observed data X and an external variable y that we are trying to predict, usually called “target” or “labels”. Most often, y is a 1D array of length n_samples.\n",
    "\n",
    "If the prediction task is to classify the observations in a set of finite labels, in other words to “name” the objects observed, the task is said to be a **classification** task. On the other hand, if the goal is to predict a continuous target variable, it is said to be a **regression** task.\n",
    "\n",
    "Clustering, which we've just done with K means, is a type of *unsupervised* learning similar to classification. Here, the difference is that we'll be using the labels in our data in our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.\" (Wikipedia)\n",
    "\n",
    "We've seen one classification example already, the iris dataset. In this dataset, iris flowers are classified based on their petal and sepal geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "pca_plot(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dataset with more features is the wine classification dataset, which tries to determine the original cultivar, or plant family, of three different Italian wines. A chemical analysis determined the following samples:\n",
    "\n",
    "1. Alcohol\n",
    "2. Malic acid\n",
    "3. Ash\n",
    "4. Alcalinity of ash  \n",
    "5. Magnesium\n",
    "6. Total phenols\n",
    "7. Flavanoids\n",
    "8. Nonflavanoid phenols\n",
    "9. Proanthocyanins\n",
    "10. Color intensity\n",
    "11. Hue\n",
    "12. OD280/OD315 of diluted wines\n",
    "13. Proline\n",
    "\n",
    "which can be used to classify the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "pca_plot(wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final and more difficult dataset is a sample from the National Institute of Standards and Technology (NIST) dataset on handwritten numbers. A modified and larger version of this, Modified NIST or MNIST, is a current standard benchmark for state of the art machine learning algorithms. In this problem, each datapoint is an 8x8 pixel image (64 features) and the classification task is to label each image as the correct number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:8]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Label: %i' % label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_plot(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.\" (Wikipedia)\n",
    "\n",
    "In regression, each set of features doesn't correspond to a label but rather to a value. The task of the regression algorithm is to correctly predict this value based on the feature data. One way to think about regression and classification is that regression is continuous while classification is discrete.\n",
    "\n",
    "Scikit learn also comes with a number of sample regression datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is the boston housing data. This regression dataset is used to predict the median value of a home in the suburbs of Boston given information about the neighborhood and house:\n",
    "\n",
    "1. CRIM      per capita crime rate by town\n",
    "2. ZN        proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS     proportion of non-retail business acres per town\n",
    "4. CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "6. RM        average number of rooms per dwelling\n",
    "7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "8. DIS       weighted distances to five Boston employment centres\n",
    "9. RAD       index of accessibility to radial highways\n",
    "10. TAX      full-value property-tax rate per $10,000\n",
    "11. PTRATIO  pupil-teacher ratio by town\n",
    "12. B        1000(Bk - 0.63)^2 where Bk is the proportion of African Americans by town\n",
    "13. LSTAT    lower status of the population\n",
    "\n",
    "Target: MEDV     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "Here the y axis indicates the value that we want to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "y = boston.target\n",
    "plt.figure(figsize=(20,20))\n",
    "features = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n",
    "for i in range(12):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.scatter(boston.data[:, i], y, edgecolors=(0, 0, 0));\n",
    "    plt.title('Feature: %s' % features[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our final example dataset, health metrics of diabetes patients were measured and then the progress of their diabetes was quantitatively measured after 1 year. The features are:\n",
    "\n",
    "1. age\n",
    "2. sex\n",
    "3. body mass index\n",
    "4. average blood pressure\n",
    "+ 5-10 six blood serum measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "y = diabetes.target\n",
    "plt.figure(figsize=(20,20))\n",
    "features = [\"AGE\", \"SEX\", \"BMI\", \"BP\", \"BL1\", \"BL2\", \"BL3\", \"BL4\", \"BL5\", \"BL6\"]\n",
    "for i in range(10):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.scatter(diabetes.data[:, i], y, edgecolors=(0, 0, 0));\n",
    "    plt.title('Feature: %s' % features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: UCI datasets</b>\n",
    "     <ul>\n",
    "      <li>\n",
    "Many of these datasets originally come from the UCI Machine Learning Repository. Visit https://archive.ics.uci.edu/ml/index.php and select a dataset. What is the dataset describing? What are the features? Is it classification or regression? How many data samples are there?\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
